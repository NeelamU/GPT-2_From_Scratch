{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMi3+e7Mpq3PLuIpO7QKrlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeelamU/GPT-2_From_Scratch/blob/main/gpt_2_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "lfgY5ZROG2yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode the dataset"
      ],
      "metadata": {
        "id": "mKdPZNTl1Jxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Dataset --> X_train, y_train block\n",
        "def encode_dataset(vocab, sentences):\n",
        "  encodings = np.empty((len(sentences), ), dtype=object)\n",
        "  for x, sentence in enumerate(sentences):\n",
        "    encodings[x] = encode(sentence)\n",
        "\n",
        "\n",
        "\n",
        "  train_size = int(math.floor(0.9*len(encodings)))\n",
        "  X_train = np.empty((train_size, ), dtype = object)\n",
        "  y_train = np.empty((train_size, ),dtype = object )\n",
        "  print(train_size)\n",
        "\n",
        "\n",
        "  for x, sentence in enumerate(encodings[:train_size]): #gonna have to torchify this\n",
        "    if len(encodings[x])>1: ## if only 1 word sentence then X_train will be None\n",
        "      X_train[x] = encodings[x][:-1]\n",
        "      y_train[x] = encodings[x][1:]\n",
        "\n",
        "\n",
        "  return X_train, y_train\n"
      ],
      "metadata": {
        "id": "vhaD1RsaBbLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## BLOCK TO CALL BROWN DATASET, Define global encode/decode, Instantiate X_train, y_train\n",
        "\n",
        "nltk.download('brown')\n",
        "text = brown.raw()  # Get the raw text from the Brown corpus\n",
        "sentences = brown.sents()\n",
        "sentences = [' '.join(each_sent) for each_sent in sentences]\n",
        "sentences = ' '.join(sentences)\n",
        "print(sentences[:1000])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsf_LOnWBD_H",
        "outputId": "ff3a02f3-8a9f-4c50-a979-04d67fa43065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place . The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted . The September-October term jury had been charged by Fulton Superior Court Judge Durwood Pye to investigate reports of possible `` irregularities '' in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr. . `` Only a relative handful of such reports was received '' , the jury said , `` considering the widespread interest in the election , the number of voters and the size of this city '' . The jury said it did find that many of Georgia's registration and election laws `` are outmoded or inadequate and often ambiguous '' . It recommended that Fulton legislators act `` to have th\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### create vocab:\n",
        "words = sentences.split()\n",
        "vocab = sorted(list(set(words)))\n",
        "text = sentences\n",
        "vocab_size = len(vocab)\n",
        "print(vocab[500:550])\n",
        "print(len(vocab))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHU0TDxqaIq6",
        "outputId": "f8627514-e0dd-4d29-abad-e639777dbffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['1-6', '1-701', '1-a', '1-degree', '1-degree-C', '1-hp', '1-inch', '1-ml', '1-o', '1-ton', '1.0', '1.0-mg.', '1.00', '1.07', '1.09.3', '1.1', '1.10.1', '1.10.4', '1.10.8', '1.2', '1.23', '1.24', '1.25', '1.25%', '1.25-cm', '1.5', '1.58', '1.8', '1.8%', '1/16', \"1/16''\", '1/2', \"1/2''\", '1/2-inch', '1/20th', '1/3', '1/4', \"1/4''\", '1/4-inch', '1/50th', \"1/8''\", '1/8-inch', '1/c', '10', '10%', \"10''\", '10,000', '10,000,000', '10,500', '10,517']\n",
            "56057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Tokenizer"
      ],
      "metadata": {
        "id": "E1p1xxrv1PEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## tokenize the input\n",
        "\n",
        "char_to_num = {ch:i for i, ch in enumerate(vocab)}\n",
        "num_to_char = {i:ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "encode = lambda e: [char_to_num[ch] for ch in e.split()]\n",
        "decode = lambda d:' '.join([num_to_char[ch] for ch in d])\n",
        "\n",
        "print(sentences[:102])\n",
        "print(encode(text[:102]))\n",
        "print(decode(encode(text[:102])))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJhhgD8FEP6T",
        "outputId": "1aba0a10-d904-4cef-c3ce-b0c02654da08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produce\n",
            "[17590, 8186, 5895, 8687, 10432, 47061, 8136, 20448, 36445, 41065, 3264, 45363, 44000, 29542, 44134]\n",
            "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produce\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = torch.tensor(encode(sentences), dtype = torch.long)\n",
        "print(dataset[:1000])\n",
        "\n",
        "split_percentage = 0.9\n",
        "n = int(split_percentage*len(dataset))\n",
        "\n",
        "train_set = dataset[:n]\n",
        "val_set = dataset[n:]"
      ],
      "metadata": {
        "id": "7ZZF_VzJG5Gw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128 # num characters in each sentence passed into model\n",
        "batch_size = 32 # how many batches of these sentences\n",
        "embed_size = 384\n",
        "head_size = 64\n",
        "num_heads = 6\n",
        "num_blocks = 6\n",
        "dropout = 0.2\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZAOpn2kSHmmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_set if split == 'train' else val_set\n",
        "  ints = torch.randint(len(data) - block_size, (batch_size, )) # take 4 random integers from dataset\n",
        "\n",
        "  inputs = torch.stack([data[x:x+block_size] for x in ints]) # batch_size, block_size\n",
        "  labels = torch.stack([data[x+1:x+block_size+1] for x in ints])\n",
        "  inputs = inputs.to(device)\n",
        "  labels = labels.to(device)\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "\n",
        "ins, outs = get_batch('train')\n",
        "print(ins)\n",
        "print(outs)\n",
        "\n"
      ],
      "metadata": {
        "id": "y9fl3keOJJP9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fddbc193-faa5-47a6-a055-538d636d114f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[47523,   393, 20953,  ..., 51924,  9791, 37657],\n",
            "        [28265, 20963, 22651,  ..., 52131, 52017, 50101],\n",
            "        [41815, 35924, 54762,  ...,   405, 17905, 34072],\n",
            "        ...,\n",
            "        [42525, 20526, 55603,  ..., 55215, 21545, 41065],\n",
            "        [51924, 38785, 33900,  ..., 51924,  7209, 16053],\n",
            "        [39219, 41759, 34450,  ..., 55603, 51924,  2730]], device='cuda:0')\n",
            "tensor([[  393, 20953,   393,  ...,  9791, 37657, 35400],\n",
            "        [20963, 22651, 41519,  ..., 52017, 50101, 35400],\n",
            "        [35924, 54762,   393,  ..., 17905, 34072, 34001],\n",
            "        ...,\n",
            "        [20526, 55603, 51924,  ..., 21545, 41065, 51924],\n",
            "        [38785, 33900, 35400,  ...,  7209, 16053, 35400],\n",
            "        [41759, 34450,   405,  ..., 51924,  2730,   405]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implement a Single Head"
      ],
      "metadata": {
        "id": "AuUQw8yy1gqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    self.key = nn.Linear(embed_size, head_size, bias = False) # need bias = False to preserve auto regression (i.e. 0s in diagonals)\n",
        "    self.query = nn.Linear(embed_size, head_size, bias = False)#key @ query == attention scores for every token with every other token for every context length for each of the batches\n",
        "    self.value = nn.Linear(embed_size, head_size, bias = False)\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size))) ## !!!\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, res_stream ):\n",
        "    batch, block, embed = res_stream.shape\n",
        "\n",
        "    key = self.key(res_stream) # res_stream is batch x block x embed\n",
        "    query = self.query(res_stream)\n",
        "\n",
        "\n",
        "\n",
        "    # due to masking, the attention scores are autoregressive\n",
        "    # that is to say, each context size, 1 to block size, can't see the future tokens\n",
        "    attn_scores = query @ key.transpose(-2, -1) * embed **-0.5 #batch x block x block, and * head_size is the sqrt(dk)\n",
        "    attn_scores = attn_scores.masked_fill(self.tril[:block, :block] == 0, float('-inf')) # lower triangle so tokens of each size are attended to, masked fill makes 0s to -infs\n",
        "    attn_scores = F.softmax(attn_scores, dim = -1)\n",
        "    attn_scores = self.dropout(attn_scores)\n",
        "\n",
        "    values = self.value(res_stream)\n",
        "    # DROPOUT HERE!\n",
        "    # head_out preserves the autoregressive nature by virtue of matrix multiplication with attn_scores\n",
        "    # that is, past values for tokens are matmuled, by future tokens are simply zeroed out do to the masking in attn_scores\n",
        "    head_out = attn_scores @ values # batch x block x head\n",
        "\n",
        "    return head_out\n"
      ],
      "metadata": {
        "id": "BpwT-QCbn6C1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multi Head Attention"
      ],
      "metadata": {
        "id": "noeAMRYd1lgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, num_heads, head_size):\n",
        "    super().__init__()\n",
        "\n",
        "    #instantiate num_heads heads into a nn.ModuleList (a list for modules)\n",
        "    self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "    self.to_stream = nn.Linear(num_heads * head_size, embed_size) #bias False?\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    ## ADD DROPOUT\n",
        "  def forward(self, res_stream):\n",
        "\n",
        "    # returns a list of the output of the heads concatenated together\n",
        "    # since this feed forwards each head, and the output of Head.forward() is head_out of size batch, block, head_size --\n",
        "    # and since torch.cat is operating on the last dimension (-1 -- head_size) --\n",
        "    # the new multi head dimensional output will be batch, block, num_head * head_size\n",
        "    # essentially each head concatenates what it learns from attn_scores @ values\n",
        "    # then a linear layer projects the auto regression from multi heads to embedding size to be added back to stream\n",
        "    multi_heads = torch.cat([h(res_stream) for h in self.heads], dim = -1) # batch, block, num_head * head_size\n",
        "    attention_out = self.dropout(self.to_stream(multi_heads)) # batch x block x embed_size\n",
        "\n",
        "    return attention_out\n",
        "\n"
      ],
      "metadata": {
        "id": "eSr36VEOx0yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The 4x MLP  Block"
      ],
      "metadata": {
        "id": "otMASGYn1qo_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForwardMLP(nn.Module):\n",
        "  def __init__(self, embed_size):\n",
        "    super().__init__()\n",
        "    self.MLP = nn.Sequential(\n",
        "        nn.Linear(embed_size, embed_size*4),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(embed_size*4, embed_size),\n",
        "        nn.Dropout(dropout)\n",
        "        # ADD DROPOUT\n",
        "    )\n",
        "\n",
        "  def forward(self, attention_out):\n",
        "    MLP_out = self.MLP(attention_out) # size batch x block x embed_size\n",
        "    return MLP_out\n",
        "\n"
      ],
      "metadata": {
        "id": "vJ2k8IVDzqE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A Single Transformer MHA+MLP Block"
      ],
      "metadata": {
        "id": "D0rvLBq61xHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self, num_heads, head_size, embed_size ):\n",
        "    super().__init__()\n",
        "    self.multihead_attn_layer = MultiHeadAttention(num_heads, head_size)\n",
        "    self.MLP_layer = FeedForwardMLP(embed_size)\n",
        "    self.layernorm1 = nn.LayerNorm(embed_size)\n",
        "    self.layernorm2 = nn.LayerNorm(embed_size)\n",
        "    #self.LayerNorm\n",
        "\n",
        "  def forward(self, res_stream ):\n",
        "\n",
        "    attention_out = self.multihead_attn_layer(self.layernorm1(res_stream)) + res_stream ### + res_stream implements residual stream\n",
        "    MLP_FF = self.MLP_layer(self.layernorm2(attention_out)) + attention_out ## + attention_out implements residual stream\n",
        "\n",
        "    return MLP_FF\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Lle33kbCzL_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all Together"
      ],
      "metadata": {
        "id": "1Vz_WttP12DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, embed_size) # embeddings are vocab_size\n",
        "    self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "\n",
        "    blocks = [Block(num_heads, head_size, embed_size) for _ in range(num_blocks)]\n",
        "    self.TransformerBlocks = nn.Sequential(*blocks)\n",
        "\n",
        "    # self.TransformerBlocks = nn.Sequential(*[Block(num_heads, head_size, embed_size)] for _ in range(num_blocks))\n",
        "    # self.TransformerBlock1 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock2 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock3 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock4 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock5 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock6 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock7 = Block(num_heads, head_size, embed_size)\n",
        "    # self.TransformerBlock8 = Block(num_heads, head_size, embed_size)\n",
        "\n",
        "    self.layernorm_final = nn.LayerNorm(embed_size)\n",
        "\n",
        "    self.unembedding = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "  def forward(self, inputs, labels):\n",
        "    batch, block = inputs.shape\n",
        "\n",
        "    # embedding and position encoding\n",
        "\n",
        "    embeddings = self.token_embedding_table(inputs) # of size batch_size, block_size, embed_size, in this case embed_size = vocab_size\n",
        "    positions = self.position_embedding_table(torch.arange(block, device = device)) # this results in block_size, embed_size\n",
        "\n",
        "    # print(embeddings.shape, 'emb')\n",
        "    # print(positions.shape, 'position')\n",
        "    res_stream = embeddings + positions # batch x block x embed -- different shapes add to make 4, 8, 32 i.e. 8, 32 + 4, 8, 32 = 4, 8, 32\n",
        "    # TRANSFORMER BLOCK (multi-head attention, MLP layer, LayerNorm)\n",
        "    to_stream = self.TransformerBlocks(res_stream) # to_stream size -- batch x block x embed\n",
        "\n",
        "    to_logits = self.layernorm_final(to_stream) # last layernorm\n",
        "    #unembedding -- from residual stream (embedding) to vocab_size logits\n",
        "    logits = self.unembedding(to_logits) # batch x block x vocab\n",
        "    # B, T, C = logits.shape\n",
        "    # logits = logits.view(B*T, C)\n",
        "    # targets = labels.view(B*T)\n",
        "    # loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "    return logits\n",
        "\n",
        "  def generate(self, input  , max_new_tokens):\n",
        "    #index is batch_size * len_seq\n",
        "    for i in range(max_new_tokens):\n",
        "\n",
        "      input_forward = input[:, -block_size:]\n",
        "      logits = self(input_forward, input_forward ) # input = labels is redundant\n",
        "\n",
        "      logits = logits[:, -1, :] #batch, embed taken on the last character prediction\n",
        "\n",
        "      probabilities = F.softmax(logits, dim = -1) # -1 means take softmax on last dimension i.e the embed dimension\n",
        "\n",
        "      next_idx = torch.multinomial(probabilities, num_samples = 1)  # get idx of max probability\n",
        "\n",
        "      input = torch.cat((input, next_idx), dim = 1) # size batch, block_size + 1\n",
        "\n",
        "\n",
        "    return input\n"
      ],
      "metadata": {
        "id": "rCM5F2Kh3OZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train it!"
      ],
      "metadata": {
        "id": "SkuQ6YLU15-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 1e-4\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "model = model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "neK9jToAZZ6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 128 # num characters in each sentence passed into model\n",
        "batch_size = 32 # how many batches of these sentences\n",
        "embed_size = 768\n",
        "head_size = 256\n",
        "num_heads = 6\n",
        "num_blocks = 12\n",
        "dropout = 0.4\n"
      ],
      "metadata": {
        "id": "7VPQO9g7R97H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAINING LOOP\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "loss = 0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "steps = 5000\n",
        "eval_iters = 25\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "for step in range(steps):\n",
        "  if step % 200 == 0:\n",
        "    losses_tr = []\n",
        "    losses_val = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for split in ['train', 'val']:\n",
        "        for k in range(eval_iters):\n",
        "          inputs, labels = get_batch(split)\n",
        "\n",
        "          logits =  model(inputs, labels)\n",
        "\n",
        "\n",
        "          logits = logits.permute(0, 2, 1)\n",
        "          loss = loss_func(logits, labels)\n",
        "          if split == 'train':\n",
        "            losses_tr.append(loss.cpu())\n",
        "          else:\n",
        "            losses_val.append(loss.cpu())\n",
        "\n",
        "      loss_tr = np.array(losses_tr).mean()\n",
        "      loss_val = np.array(losses_val).mean()\n",
        "\n",
        "      print(f\"step {step}: train loss {loss_tr:.4f}, validation loss {loss_val:.4f}\")\n",
        "\n",
        "  model.train()\n",
        "\n",
        "\n",
        "  inputs, labels = get_batch('train')\n",
        "\n",
        "  logits =  model(inputs, labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  logits = logits.permute(0, 2, 1)\n",
        "  loss = loss_func(logits, labels)\n",
        "\n",
        "\n",
        "  optimizer.zero_grad(set_to_none = True)\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKbA9hDq9vPZ",
        "outputId": "84e98d5a-ce55-4518-cfaa-19eb4b901790"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "53.797625 M parameters\n",
            "step 0: train loss 11.1013, validation loss 11.0899\n",
            "step 200: train loss 6.9925, validation loss 6.8764\n",
            "step 400: train loss 6.6708, validation loss 6.5494\n",
            "step 600: train loss 6.4901, validation loss 6.3577\n",
            "step 800: train loss 6.3685, validation loss 6.2104\n",
            "step 1000: train loss 6.2503, validation loss 6.1770\n",
            "step 1200: train loss 6.1446, validation loss 6.0530\n",
            "step 1400: train loss 6.0867, validation loss 6.0351\n",
            "step 1600: train loss 5.9610, validation loss 5.9866\n",
            "step 1800: train loss 5.8928, validation loss 5.9251\n",
            "step 2000: train loss 5.8202, validation loss 5.9363\n",
            "step 2200: train loss 5.7455, validation loss 5.8734\n",
            "step 2400: train loss 5.6631, validation loss 5.8834\n",
            "step 2600: train loss 5.6145, validation loss 5.8218\n",
            "step 2800: train loss 5.5776, validation loss 5.8178\n",
            "step 3000: train loss 5.5113, validation loss 5.8005\n",
            "step 3200: train loss 5.4438, validation loss 5.8134\n",
            "step 3400: train loss 5.3754, validation loss 5.8254\n",
            "step 3600: train loss 5.3545, validation loss 5.8088\n",
            "step 3800: train loss 5.2843, validation loss 5.8480\n",
            "step 4000: train loss 5.2358, validation loss 5.7907\n",
            "step 4200: train loss 5.1700, validation loss 5.7823\n",
            "step 4400: train loss 5.1331, validation loss 5.7797\n",
            "step 4600: train loss 5.0688, validation loss 5.7806\n",
            "step 4800: train loss 5.0231, validation loss 5.7857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate some Text"
      ],
      "metadata": {
        "id": "k5hbejFi199T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "testin, testout = get_batch('train')\n",
        "the_in = testin.cpu().numpy().tolist()\n",
        "for each in the_in:\n",
        "  print(decode(each))\n",
        "  break\n",
        "\n",
        "out = model.generate(testin, 20\n",
        "                     )\n",
        "print('\\n\\n\\n')\n",
        "print('AI GENERATED TEXT:')\n",
        "print('\\n\\n\\n')\n",
        "for each in out:\n",
        "  sentence_pred = each.cpu().numpy().tolist()\n",
        "  print(decode(sentence_pred))\n",
        "  break # just one of the block_size samples"
      ],
      "metadata": {
        "id": "GgHtHJ_NazvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jMTn5xq2xMCX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}